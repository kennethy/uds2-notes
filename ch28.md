## 28. Upstream resiliency

### 28.1 Load shedding
- The operating system has a connection queue per port with a limited capacity.
  - New connection attempts are rejected when the number of connections reach the capacity.
  - Excess requests should be rejected when a server is operating at capacity. This allows resources to be used for the requests the server is already processing.
  - Load shedding: the server keeps track of the concurrent requests it is handling, and it fails fast incoming requests (returns status 503, service unavailable) when it's overloaded.
    - The fail-fast mechanism could also account for the priority of the requests.
- Server might still have to pay the price of opening a TLS connection and reading the request just to reject it.

### 28.2 Load leveling
- An alternative to load shedding, in which clients do not expect a prompt response from the server.
- A message channel is used between the client and the server.
  - Decouples clients from the servers, and allow servers to process requests at their own pace.
  - Potential problem: a large backlog could build up.
- Both load-shedding and load-leveling do not address an increase in load directly, and thus they are typically combined with auto-scaling.

### 28.3 Rate limiting
- A mechanism to reject a request when a specific quota is reached.
- In HTTP APIs, status code 429 (too many requests) are typically used when rate limit is reached.
  - Headers could include additional details about which quota is reached or when the request can be retried (via `Retry-After` header).
- When a request is rate limited by the API key, the server still has to open a TLS connection just to read the API key.
- Rate limiting sheds traffic based on the global state of the system, whereas load sheeding rejects traffics based on the local state of a process.

### 28.3.1 Single-process implementation
- A naive implementation of rate limiting is to use a doubly-linked list per API key, where each list stores the timestamps of the last N requests received. This is memory expensive as it grows with the number of requests received.
  - Mitigation: divide time into buckets and keep track of how many requests have been seen for each bucket.
- A sliding window can be used to keep track of the number of requests within it.
  - A weighted sum of the buckets' counter is calculated (the two buckets the sliding window is overlapping with).
  - For example: sliding window overlaps with A (60%, counter of 3), and with B (40%, counter of 1), 0.6 * 3 + 0.4 * 1 ~= 2.2.
  - Only requires two counters since the sliding window can only be overlapping with two windows at a time. 

### 28.3.2 Distributed implementation
- A shared data store is needed when a request can be served by more than one process, since the quota should be enforced for all processes.
- Transaction could be used to prevent concurrent updates to the data store, but it could be too slow.
  - Mitigation: use the single atomic `get-and-increment` operation, or `compare-and-swap`. Rather than updating the data store on each request, the process can batch bucket updates in memory for some time and flush them asynchronously to the data store at the end of it (reducesthe shared state's accuracy but it might be a good tradeoff depending on the business requirement).

### 28.4 Constant work
- Multi-modale behaviour: when overload, configuration changes, or faults force an application to behave differently.
  - Rule of thumb: aim to reduce the number of modes.
- Anti-fragile system: when the system performs the same amount of work under high load as under average load. If there is any variation under stress, it should be because the system is performing better, not worse.
  - For example: instead of broadcasting a user settings update to the data plane, the control plane periodically dump the updates to a file in a high available file storage, and the data plane periodically read from the file storage.
  - Empty configuration slots can be pre-allocated to support up to a certain number of users. This stabilizes the changes required when the number of users grow. This approach is typically used in cellular architectures.
- Some benefits:
  - Data plane performs the same amount of work in bulk; system is reliable and predictable.
  - Self-healing since corrupted data will be updated in the next run.