## 31. Monitoring
- Blackbox monitoring: report whether a service is up; useful for detecting symptoms of a failure.
  - Synthetics: send test requests to external APIs and monitor their health.
- Whitebox monitoring: report whether a specific feature is working as expected; useful for identifying the root cause of the failure.

### 3.1. Metrics
- Metric: time series of raw measurements of resource usage or behaviour (e.g CPU utilization, number of requests).
- A service should emit metrics about its load,internal state, dependencies availability, and performance.
- Time series samples can be pre-aggregated over fixed period time periods.
  - Can further reduce the ingestion costs by having the local telemetry agents to pre-aggregate metrics client-side, but with the tradeoff that we loes the ability to re-aggregate metrics after ingestion.
- Metrics are usually persisted in a pre-aggregated form in a data store specialized for efficient time series storage.

### 3.2 Service-level indicators
- Service-level indicators (SLI) measures the different aspects of the level of service provided by a service to its users (e.g. response time, error rate, throughput).
  - Typically aggregated over a rolling time window.
  - Best defined as ratios of two metrics: "good" events over the total number of events.
  - Examples:
    - Response time: fraction of requests that completed faster than a given threshold.
      - Best to be presented with percentiles.
      - If p99 is 1 second, it means 99% of the requests completed less than or equal to 1 second.kkk
    - Availability: number of success requests over the total number of requests.
  - Pick metrics that best represents the users' experience.
  - Long-tail latencies:  upper percentiles of a response time distribution.
- Little's Law: long term averagenumber L of customers in a stationary system is equal to the long-term average effective arrival rate of λ multipled by the average time W that a ustomer spends in the system.

### 31.3 Service-level ojectives
- SLO defines a range of acceptable values for an SLI within which the service is considered to be in a healthy state.
- Service owners can also use SLOs to define a service-level agreemenet (SLA) with their users (contractual agreement).
- Error budget: number of failures that can be tolerated.
- chaos testing: periodically inject controlled failures into production, and ensure the dependencies can cope with the targeted service level.

### 31.4 Alerts
- Alerts should be actionable in order for them to be useful.
- Monitor the SLO's error budget and trigger an alert when a large poetion of it has been consumed.
- Precision: fraction of significant events over the total number of alerts (true pos/(true pos + false pos)). How many anomolies are actually real anomalies.
- Recall: ratio of significant events that triggered an alert (true pos/(true pos + false neg)). It indicates how well the system is at catching all true anomolies.
- There is often a tradeoff between precision and recall.
- Burn rate: the percentage of the error budget consumed over the percentage of the SLO time window that has elapsed — the rate of exhaustion of the error budget.

### 31.5 Dashboards
- Before creating a dashboard, we must first decide who the audience is and what they are looking for.
- Types of dashboards
  - SLO dashboard: provides visibility into the health of the system represented by its SLOs.
  - Public API dashboard
  - Service dashboard: display service-specific implementation details. Could also contain metrics of upstream or downstream dependencies (load balancers, message queues, data stores).

### 31.5.1 Best practices
- Define dashboards and charts with domain-specific language and version controlling them just like code.
- Use UTC as default timeone to ease the communication between people located in different regions.
- All charts should use the same time resolution.
- A chart should include information like:
  - Description of the chart with links to runbooks.
  - Vertical line for each relevant deployment.
- Best practice is to emit a metric value using the value of zero; this avoids the confusion on whether the service has actually stopped eimitting metircs versis due to a bug.
- First step of an alert is to mitigate it as opposed to fixing the underlying root cause.
- Post-mortem helps to understand the incident's root cause and the necessary fixes in order to prevent it from happening again.