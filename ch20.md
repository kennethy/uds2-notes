## 20. Caching
- Cache: a high-speed storage layer that temporarily buffers responses from an origin.
- Hit ratio: how often an item is returned from the cache.
  - Depends on the total number of cacheable objects (the fewer the better), and the likelihood of accessing the same object repeatedly (the higher, the better).
- Caching is an optimization: without a scalable architecture, the origin would not be able to withstand the load without the cache fronting it.

### 20.1 Policies
- With a cache miss, the missing object has to be requested from the origin.
  - Side cache: when the missing item is fetched from the origin, and the application updates the cache.
  - Inline cache: the cache fetches the missing item instead of the application.
- Eviction policies: with limited capacity, items may need to be evited to make room for new ones.
  - Expiration policy: each item has a time-to-live (TTL). The higher the TTL, the higher the hit ratio, but the higher the likelihood of serving stale and inconsistent data.
    - Can be deferred the next time the entry is requested. If the origin is unavailable, it might be resilient to return an object with an expired TTL rather than an error.
    - Cache invalidation is difficult to implement in practice, if the query is based on data that spans on a lot of records.

### 20.2 Local cahe
- Simplest way to implement a cache is to co-locate it with the client using a in-memory hash table.
  - Same objects are duplicated across caches, wasting resources (number of requests for the object grows with the number of clients).
  - Consistency issues will also inevitably arise.
  - Thundering herd: issue exacerbated on clients restart. All clients' cache need to be populated again. Downstream origin is hit with a spike of requests.
    - Coalescing: at most one outstanding request per client to fetch  for a specific object.

### 30.3 External cache
- A dedicated service is used for cahing objects (typically in memory).
  - Example: Redis, Memcached
- Can use replication for throughput and partitioning for size.
  - Redis: data is automatically partitioned across multiple nodes using a leader-follower protocol.
- Reduces consistency issue since the cache is shared among clients, and there is only a single version of the same object for all clients.
- Unlike internal caches, the number of times an object is requested from the origin does not grow with the number of clients.
- Comes with additional maintenance cost.
- Higher latency because a network call is requiredto the external cache.
- Problem: external cache being down may lead to cascading failures.
  - Clients should use in-process cache as a defence.
- Caching is an optimization, the system needs to withstand the additional load in the event the cache is unavailable.