## Chapter 11. Coordination avoidance
- The need of total order leads to a scalability bottleneck since updates need to be processed by a single process in a sequential order.

### 11.1 Broadcast protocols
- TCP offers point-to-point communication (**unicast**).
- A broadcast protocol delivers a message to a group of processes (**multicast**). It is built on top of a unicast one, and the difficulty lies in handling the scenarios when senders and receivers can crash at any time.
  - It can be characterized by the guarantee it provides.
  - **Best-effort broadcast** guarantees that if the sender doesn't crash, the message will be sent to all non-faulty processes. It can be achieved by sending the message to each process one by one over a reliable link.
  - **Reliable-broadcast** guarantees the message will be eventually delivered to all non-faulty processes, even if the sender crashes. It can be achieved by having each process to retransmit the message to the rest of the group the first time it is delivered (also known as eager reliable broadcast).
    - It can be costly since there will be N^2 messages.
    - **Gossip broadcast protocol**: the message is retransmitted to a random subset of processes. It's a probabilistic protocol that does not guarantee that a message will be delivered to all processes.
- **Total order broadcast**: guarantees all messages are delivered to all processes in the same order.

### 11.2 Conflict-free replicated data types
- **Eventual consistency** requires:
  - Eventual delivery: guarantees that every update at a replica is eventually applied at all replicas
  - Convergence: guarantees the replicas that have applied the same updates eventually reach the same state.
- When messages are not delivered in the same order, one way to reconcile this divergence is to use consensus to make a decision that all replicas need to agree with.
- **Strong eventual consistency**:
  - Requires eventual delivery
  - Requires strong convergence: the guarantee that replicas that have applied the same update have the same state (every update is immediately persisted). For example, a deterministic approach that eliminates conflicts, like the last write wins strategy.
    - Conditions for strong convergence:
      - given an object that can be queried and updatee
      - The replica responds the local copy of the object when it receives a query
      - On an update request, the replica applies the update to its local copy then broadcasts the updated object to all replicas
      - When a replica receives a broadcast message, it merges the object from the message with its own
    - Each replica will converge to the same state if:
      - Object's possible states form a semilattice; a set that contains elements that can be partially ordered
      - The merge operation returns the least upper bound between the two object's states (therefore idempotent, commutative (a + b = b + a), and associative (a * b = b * a))
    - Objects that have the above properties are known as the conflict-free replicated data types (CRDTS).
    - Anti-entropy mechanism: when replicas use an unreliable protocol but periodically merge their states to ensure they eventually converge.
  - For example, a register is a CRDTS
    - Last-write-wins (LWW): associates a timestamp with every update to make updates totally orderable.
      - Can use lamport timetsamp to preserve the happened-before relationship.
      - Concurrent updates are handled by taking the one with the greater timestamp, which might not always be correct.
      - Concurrent updates can be sent back to the client for resolution.
    - Multi-value (MV): keeps track of all concurrent updates and return them to client for conflict resolution. Uses vector clock to detect conflicts.
  - Dynamo-style data stores use LWW or MV registers.

### 11.3 Dynamo-style data stores
- Every replica can accept write and read requests.
- When the client wants to perform a write, it sends the request to all N replicas in parallel but waits for an acknowledge from just W replicas (a write quorum).
- When the client wants to perform a read, it also sends the request to all N replicas in parallel but waits for R replies (a read quorum), and returns the most recent entry to the client.
- To resolve conflicts, entries behave like LWW or MV registers depending on the flavor.
- When W + R > N, at least one replica will return the latest version, though linearizability is not guaranteed since the write could succeeeds on less than W replicas.
- W and R are typically configured to be majority quorum (more than half of the number of replicas).
- A read-heavy workload benefits from a smaller R (when W + R > N).
- W = R = 1 for fast-write at the expensive of consistency.
- To ensure all write request is sent to a replica, two anti-entropy mechanisms are used:
  - Read-repair: implemented by the client to help bring replicas back in sync when a read is performed. The client waits for R replies, and if it sees replies with older entries, it can issue a write request with the latest entry to out-of-sync replicas.
    - Not enough to guarantee all replicas will eventually converge.
  - Replica synchronization: a continuous background mechanism that runs on every replica and periodically communicates with others to identify and repair inconsistencies.
    - Can be implemented using gossip protocol or Merkle tree hashes.
- Quorum replication can be thought of as best-effort broadcast combined with anti-entropy mechanisms to ensure all changes are propagated to all replicas.

### 11.4 The CALM Theorem
- The CALM Theorem: a program has a consistent, coordination-free distributed implementation if and only if it is monotonic.
  - If new inputs further refine the output and can't take back any prior output, like a union of a set.
  - CRDTs are monotonic.
- Non-monotonic program can retract a prior output, like variable assignment.
- Consistency in CALM doesn't refer to linearizability, the C in CAP.
- Linearizability is concerned with consistency of reads and writes, whereas CALM focuses on the consistency of the program's output. A consistent program returns the same output no matter in which order the inputs are processed.
  - For example, the order of processing write(1), write(2), write(3) matters, whereas multiple of increment(1) doesn't.
- By combining the assignment operation with a logical clock, it is possible to build a monotonic implementation.

### 11.5 Causal consistency
- To preserve the happened-before order (causal order), we can use a weaker conssitency model called causal consistency.
  - It is weaker than strong consistency but stronger than eventual consistency.
  - The strongest consistency model that enables systems to be available and partition tolerant.
- It requires processes to agree on the order of causually related operations but can disagree on the order of unrelated ones.
- Strong consistency imposes a global order that all processes need to agree with.
- Clusters of Order-Preserving Servers (COPS)
  - When the client reads a value, it adds the version (logical timestamp) of the value it received to keep track of dependencies.
  - When a client sends a write to its local replica, it adds a copy of the dependency dictionary to the request.
    - The replica assigns a version to the write, applies it, and sends back an acknowledgement with the new version assigned.
    - The update can be applied right away because of LWW.
    - The update is broadcasted asynchronously to the other replicas.
  - When a replica receives a replication message for a write, it first check whether the writer's dependencies have been committed locally. If not, it waits until the required versions appear.
  - It's possible a replica could fail after committing an update locally but before broadcasting, resulting in data loss.

### 11.6 Practical considerations
- Weaker consistency models have higher throughputs.